\documentclass[german,plainarticle,hyperref,utf8]{zihpub}
\author{Daniel Körsten}
\title{Komplexpraktikum Paralleles Rechnen - MPI parallele Programmierung}
\matno{4690396}
\betreuer{Dr. Robert Schöne}

\usepackage{listings}
\usepackage{color}
\usepackage{caption}
\usepackage{float}
\usepackage{pgf}
\usepackage{appendix}
\usepackage{svg}

\definecolor{mygreen}{rgb}{0,0.6,0}
\definecolor{mygray}{rgb}{0.5,0.5,0.5}
\definecolor{mymauve}{rgb}{0.58,0,0.82}

\lstset{ 
	backgroundcolor=\color{white},   % choose the background color; you must add \usepackage{color} or \usepackage{xcolor}; should come as last argument
	basicstyle=\footnotesize,        % the size of the fonts that are used for the code
	breakatwhitespace=false,         % sets if automatic breaks should only happen at whitespace
	breaklines=true,                 % sets automatic line breaking
	captionpos=b,                    % sets the caption-position to bottom
	commentstyle=\color{mygreen},    % comment style
	deletekeywords={...},            % if you want to delete keywords from the given language
	escapeinside={\%*}{*)},          % if you want to add LaTeX within your code
	extendedchars=true,              % lets you use non-ASCII characters; for 8-bits encodings only, does not work with UTF-8
	firstnumber=1,                % start line enumeration with line 1000
	frame=single,	                   % adds a frame around the code
	keepspaces=true,                 % keeps spaces in text, useful for keeping indentation of code (possibly needs columns=flexible)
	keywordstyle=\color{blue},       % keyword style
	language=Octave,                 % the language of the code
	morekeywords={*,...},            % if you want to add more keywords to the set
	numbers=left,                    % where to put the line-numbers; possible values are (none, left, right)
	numbersep=5pt,                   % how far the line-numbers are from the code
	numberstyle=\tiny\color{mygray}, % the style that is used for the line-numbers
	rulecolor=\color{black},         % if not set, the frame-color may be changed on line-breaks within not-black text (e.g. comments (green here))
	showspaces=false,                % show spaces everywhere adding particular underscores; it overrides 'showstringspaces'
	showstringspaces=false,          % underline spaces within strings only
	showtabs=false,                  % show tabs within strings adding particular underscores
	stepnumber=1,                    % the step between two line-numbers. If it's 1, each line will be numbered
	stringstyle=\color{mymauve},     % string literal style
	tabsize=2,	                   % sets default tabsize to 2 spaces
	title=\lstname                   % show the filename of files included with \lstinputlisting; also try caption instead of title
}

\begin{document}
	\section{Aufgabenbeschreibung}
	In dieser Aufgabe soll eine MPI-parallele Variante von \texttt{Conway’s Game-of-Life} in der Programmiersprache \texttt{C} implementiert werden.\\
	Anschließend soll die Simulation mit verschieden großen Feldern und Anzahl von ranks durchgeführt und verglichen werden.
	
	\subsection{Conway’s Game-of-Life}
	Das Game-of-Life ist ein vom Mathematiker John Horton Conway entworfenes Simulationsspiel \cite{gardner}. Es basiert auf einem zellulären Automaten. Häufig handelt es sich um ein zweidimensionales Spielfeld, jedoch ist auch eine dreidimensionale Simulation möglich.
	
	Das Spiel besteht dabei aus einem Feld mit einer festgelegten, möglichst großen, Anzahl an Zeilen und Spalten. Eine Zelle kann dabei entweder Tot oder Lebendig sein. Dieses Spielfeld wird mit einer zufälligen Anfangspopulation initialisiert.
	
	Ein Sonderfall stellen die Ecken und Kanten des Feldes dar, da dort nach den Spielregeln das Verhalten nicht festgelegt ist. Die Aufgabenstellung gibt vor, dass das Spielfeld Torus-förmig sein soll. Alles, was das Spielfeld auf einer Seite verlässt, kommt auf der gegenüberliegenden Seite wieder herein.
	
	Anschließend wird durch die Befolgung der Spielregeln die nächste Generation berechnet. Dafür betrachtet man jede Zelle und ihre acht Nachbarn, um ihre Entwicklung zu berechnen. Es gelten folgende Spielregeln:
	\begin{enumerate}
		\item Eine lebende Zelle mit zwei oder drei Nachbarn überlebt in der Folgegeneration.
		\item Eine lebende Zelle mit vier oder mehr Nachbarn stirbt an Überpopulation, bei weniger als zwei Nachbarn an Einsamkeit.
		\item Jede tote Zelle mit genau drei Nachbarn wird in der nächsten Generation geboren.
	\end{enumerate}
	Wichtig ist, dass die Folgegeneration für alle Zellen berechnet wird und anschließend die aktuelle Generation ersetzt. Es ist also nicht möglich, die nachfolgende Generation im Spielfeld der Aktuellen zu berechnen.
	
	\subsection{Besonderheiten der Aufgabenstellung}
	Die Aufgabenstellung gibt vor, dass die Parallelisierung mittels \texttt{MPI} erfolgen soll. \texttt{MPI} ist eine API, die auf dem Message-Passing-Modell aufbaut \cite{mpi}. Das Message-Passing-Modell beschreibt dabei eine Menge von Prozessen, die mit anderen Prozessen durch das Senden und Empfangen von Nachrichten kommunizieren können.\\
	Ich habe in diesem Praktikum die Open-Source Implementierung \texttt{OpenMPI} verwendet.\\
	Im Gegensatz zu \texttt{OpenMP} benötigt MPI kein \textit{Shared-Memory System}, sondern kann auf dedizierten Rechnern ausgeführt werden, die über ein Netzwerk miteinander verbunden sind.\\
	
	MPI steht dabei für \textit{Message Passing Interface}. Konkret bedeutet das, dass die parallel laufenden Prozesse Daten über Nachrichten austauschen. Dadurch wird es möglich, das Problem aufzuteilen ("\textit{divide and conquer}"). Der Datenaustausch geschieht nicht automatisch, sondern der Programmierer muss explizit angeben, welcher Prozess zu welchem Zeitpunkt Daten an einen anderen Prozess sendet bzw. empfängt.\\
	
	Der Nachrichtenaustausch kann dabei über verschiedene Wege erfolgen.\\
	Innerhalb eines Sockets, oder auch zwischen Sockets auf einem Dual Socket Board, können Nachrichten über den gemeinsamen Speicher oder Interconnects, wie Infiniband oder Infinity Fabric mit sehr hoher Geschwindigkeit und geringer Latenz ausgetauscht werden.\\
	Kommen mehrere Compute Nodes in einem Cluster zum Einsatz, kann die Kommunikation über das Netzwerk, in der Regel über TCP/IP, erfolgen.\\
	Die genaue Testumgebung mit Kommunikationsmöglichkeiten wird später in \ref{chan} diskutiert.\\
	
	Weitere Besonderheiten:
	\begin{itemize}
		\item Für 1, 4, 16, 64, 128 und 256 ranks die Ausführungszeiten messen und vergleichen
		\item Quadratische Feldgrößen von $2048\times 2048$, $8192\times 8192$, $32768\times 32768$ und $131072\times 131072$
		\item Die Felder sollen auf die ranks aufgeteilt werden
		\item benachbarte ranks sollen möglichst nah beieinander gescheduled sein
	\end{itemize}

	\begin{description}
		\item[Begriffsklärung rank:] Jeder Prozess, der am Nachrichtenaustausch teilnimmt, muss eine eindeutige Kennung besitzen, um gezielt Nachrichten versenden und empfangen zu können.\\ MPI erstellt dabei Prozessgruppen (typischerweise eine, wenn nicht anders spezifiziert), in der ein Prozess über seinen rank identifiziert wird.\\ Der rank ist dabei eine Zahl im Intervall $\left[0, N-1\right]$, $N\dotso$ Anzahl der gestarteten Prozesse.
		
	\end{description}
	\newpage
	\section{Implementierung}
	Bei Aufgabe B \& C allokierte ich einen Speicherbereich der Größe\\ \texttt{columns * rows * sizeof(u\_int8\_t)} durch die C-Funktion \texttt{malloc()}.\\
	Der Datentyp \texttt{u\_int8\_t} benötigt dabei nur ein Byte pro Zelle und ist für die Speicherung mehr als ausreichend, da ich nur den Zustand 0 - Zelle tot und 1 - Zelle lebendig speichern muss. Ein Byte ist typischerweise die kleinste adressierbare Einheit im Speicher. Das ist auch der Grund, warum kein noch kleinerer Datentyp möglich ist.\\
	Bei MPI muss dieses Feld nun so aufgeteilt werden, dass jeder Prozess einen Teil des Feldes bearbeitet. Ich entschied mich dafür für ein \textit{chessboard} Layout, also der Aufteilung des Spielfeldes in gleich große Teilfelder.\\
	Der Grund ist, dass hier der Kommunikationsaufwand minimal wird, da jeder Prozess seinen direkten Nachbarn nur die äußerste Reihe senden (und umgekehrt von ihnen empfangen) muss.\\
	Im ersten Schritt muss MPI initialisiert werden. Das erfolgt direkt am Anfang:
	\begin{lstlisting}[language=C, caption=MPI Initialisierung mit Bestimmung von rank und cluster]
static int rank, cluster;

int main(int argc, char *argv[]) {
	// MPI
	MPI_Init(&argc, &argv);
	MPI_Comm_size(MPI_COMM_WORLD, &cluster);
	MPI_Comm_rank(MPI_COMM_WORLD, &rank);
}\end{lstlisting}

	Die beiden Variablen rank und cluster enthalten dabei direkt die wichtigsten Informationen: Den rank des Prozesses und die Gesamtzahl der Prozesse.\\
	Da die Größe des Feldes ebenfalls bekannt ist, lässt sich nun das Feld aufteilen.\\
	
	Es gilt nun zu bestimmen, in wie viele Zeilen und Spalten das Feld zerlegt werden muss. Da die Felder quadratisch sind und genau wie die ranks mit einer Zweierpotenz ausgedrückt werden können, ist eine Aufteilung immer möglich.\\
	Dafür verwende ich eine Funktion, die den \textit{Logarithmus Dualis} der rank Größe berechnet und so die Zeilen und Spalten berechnet.\\
	Dabei stellt sich ein Problem: Besitzt die Zweierpotenz einer rank Größe einen ungeraden Exponenten, bspw. $128 = 2^7$, entspricht die Anzahl der Zeilen nicht der der Spalten.\\
	In diesem Fall entschied ich mich mehr Blocks pro Zeile statt Spalte zu verwenden. Dies ist eine willkürliche Festlegung und sollte keinen Einfluss auf die Performance haben.
	\clearpage
	\begin{lstlisting}[language=C, caption=Bestimmung der Zeilen und Spalten des chessboard Layouts]
void init_chessboard() {
	u_int32_t exponent = (u_int32_t) log2((double) cluster);
	if (exponent & 1) {
		// ungerade
		blocks_per_col = pow(2, (exponent / 2));
		blocks_per_row = blocks_per_col * 2;
	} else {
		// gerade
		blocks_per_row = blocks_per_col = pow(2, (exponent / 2));
	}
	block_row = rows / blocks_per_row + 2;
	block_col = columns / blocks_per_col + 2;
	return;
}\end{lstlisting}

	In Zeile 11 und 12 bestimme ich nun direkt die Zeilen und Spalten jedes Blocks, indem ich die Zeilen und Spalten des Feldes durch die jeweilige Anzahl der Blöcke teile.\\
	Anschließend addiere ich noch 2. Wie eingangs erwähnt, benötigt jeder Block die äußerste Reihe seiner Nachbarn, um das Game-of-Life spielen zu können.\\
	Deshalb vergrößere ich das Feld um jeweils 2 Zeilen und 2 Spalten.\\
	
	Auf dieser Grundlage kann jeder rank nun sein eigenes Spielfeld initialisieren:
	\begin{lstlisting}[language=C, caption=Initialisierung der Spielfelder]
// initializing states and pointers
u_int8_t *state_1 = (u_int8_t *) malloc(block_row * block_col * sizeof(u_int8_t));
u_int8_t *state_2 = (u_int8_t *) malloc(block_row * block_col * sizeof(u_int8_t));
u_int8_t *state_in = state_1;
u_int8_t *state_out = state_2;
u_int8_t *state_tmp = NULL;
	\end{lstlisting}
	
	Um zu berücksichtigen, dass die Folgegeneration immer die aktuelle Generation ersetzt, allokiere ich einen zweiten Speicherbereich gleicher Größe. Vor dem Beginn einer neuen Berechnung vertausche ich die beiden Pointer, was dazu führt, dass die im vorhergehenden Schritt berechnete Folgegeneration zur aktuellen Generation wird und eine neue Generation berechnet werden kann.
	\clearpage
	\subsection{MPI Kommunikation}
	\subsubsection{Nachrichtenaustausch} \label{message}
	Für den Nachrichtenaustausch zwischen den Prozessen verwende ich asynchrone (\textit{non-blocking}) Kommunikation. Asynchrone Kommunikation bedeutet, dass nicht auf die Zustellung bzw. den Empfang der Nachricht gewartet wird. Die Funktionsaufrufe blockieren also nicht.\\
	Das bietet sich aus mehren Gründen an:\\
	Zum einen benötigen die Kanten und Ecken aufgrund des Torus-förmigen Spielfeldes eine separate Behandlung. Gleichzeitig müssen aber auch nur Kanten und Ecken an andere Prozesse gesendet werden.\\
	Zum anderen benötigt die Kommunikation viel Zeit, da jeder Prozess zu seinen acht Nachbarn einen \textit{send} und \textit{receive} Aufruf pro Generation und einmalig für die Initialisierung durchführen muss.
	Daher bietet sich folgender Ablauf an:
	\begin{enumerate}
		\item Kanten und Ecken berechnen
		\item Berechnete Kanten und Ecken an die acht Nachbarn senden (umgekehrt vier Kanten und vier Ecken empfangen)
		\item Während der Kommunikation im Hintergrund das innere Feld berechnen
		\item Vor der Rückkehr aus der Funktion warten, bis die Kommunikation abgeschlossen ist
	\end{enumerate}
	
	MPI bietet für die asynchrone Kommunikation die Funktionen \texttt{MPI\_Isend()} und \texttt{MPI\_Irecv()} an. Für das Warten auf den Abschluss der Kommunikation verwende ich \texttt{MPI\_Waitall()}.\\
	Beispiel für den Austausch einer Kante zwischen zwei ranks (vereinfacht dargestellt):
		\begin{lstlisting}[language=C, caption=Asynchrone Kommunikation zwischen 2 ranks]
		// calculate edges and corners
		...
		// afterwards, send and receive
		MPI_Status status[16];
		MPI_Request request[16];
		//bottom edge
		MPI_Isend(&state[(block_row - 2) * block_col + 1], block_col - 2, MPI_UINT8_T,
		          neighbour_matrix[rank_index + neighbour_col], 0,
		          MPI_COMM_WORLD, &request[0]);
		MPI_Irecv(&state[1], block_col - 2, MPI_UINT8_T,
		          neighbour_matrix[rank_index - neighbour_col], 0,
		          MPI_COMM_WORLD, &request[1]);
		// do this for the remaining 3 edges and 4 corners
		...
		// calculate inner part
		...
		// wait for communication to complete
		MPI_Waitall(16, request, status);
	\end{lstlisting}
	
	\subsubsection{Neighbour Matrix}
	
	Für den Datenaustausch über MPI ist es notwendig, dass ein Prozess mit einem gegebenen rank seine Nachbarn kennt. Da es sich um ein Torus-förmiges Spielfeld handelt, hat ein rank in der letzten Spalte als rechten Nachbarn nicht den $rank + 1$, da dieser in der ersten Spalte der nächsten Zeile liegt usw.\\
	Um dieses Problem zu lösen und die Kommunikation später zu erleichtern, generiere ich eine \textit{Neighbour Matrix}, bei der die Kanten und Ecken entsprechend gespiegelt werden.
	\begin{lstlisting}[language=C, caption=Generierung der Neighbour Matrix]
// Generierung der Matrix
// 2 Zeilen und 2 Spalten groesser, um die Spiegelung zu ermoeglichen
nb_row = blocks_per_row + 2;
nb_col = blocks_per_col + 2;
u_int32_t *neighbour_matrix = (u_int32_t *) malloc(nb_row * nb_col * sizeof(u_int32_t));

// position in neighbour_matrix
rank_index = (rank / blocks_per_col + 1) * nb_col + (rank % blocks_per_col + 1);
init_neighbour(neighbour_matrix);

// Initialisieren der Matrix mit den ranks
void init_neighbour(u_int32_t *neighbour_matrix) {
	u_int32_t z = 0;
	for (int i = 1; i < nb_row - 1; i++) {
		for (int j = 1; j < nb_col - 1; j++) {
			neighbour_matrix[i * nb_col + j] = z;
			z++;
		}
		//left
		neighbour_matrix[i * nb_col] = neighbour_matrix[(i + 1) * nb_col - 2];
		//right
		neighbour_matrix[(i + 1) * nb_col - 1] = neighbour_matrix[i * nb_col + 1];
	}
	for (int i = 1; i < nb_col - 1; i++) {
		//top
		neighbour_matrix[i] = neighbour_matrix[(nb_row - 2) * nb_col + i];
		//bottom
		neighbour_matrix[(nb_row - 1) * nb_col + i] = neighbour_matrix[nb_col + i];
	}
	// top left corner
	neighbour_matrix[0] = neighbour_matrix[(nb_row - 1) * nb_col - 2];
	// top right corner
	neighbour_matrix[nb_col - 1] = neighbour_matrix[(nb_row - 2) * nb_col + 1];
	// bottom left
	neighbour_matrix[(nb_row - 1) * nb_col] = neighbour_matrix[nb_col];
	// bottom right
	neighbour_matrix[nb_row * nb_col - 1] = neighbour_matrix[nb_col + 1];
	return;
}\end{lstlisting}

	Die Variable \textit{rank\_index} enthält dabei den Index des ranks innerhalb der Matrix. Dieser Index fungiert als konstantes Offset, um Nachrichten an seine Nachbarn adressieren zu können.


	\subsection{Daten Initialisierung} \label{data_init}
	Gemäß den Startbedingungen muss nur eines der beiden Spielfelder mit Zufallswerten initialisiert werden.\\
	Wie in \ref{message} erklärt, ist der Ablauf in mehrere Schritte unterteilt.
	
	Für die Dateninitialisierung jeder Zelle mit Null oder Eins verwende ich den Pseudo-Zufallszahlengenerator \texttt{rand\_r()}. Der \texttt{seed} setzt sich aus \texttt{time()+rank} zusammen. Damit verändert sich der \texttt{seed} in Abhängigkeit der Zeit und dem rank.\\
	Zuerst werden die Kanten und Ecken initialisiert, danach erfolgt die Kommunikation. Währenddessen wird das innere Feld initialisiert und auf den Abschluss der Kommunikation gewartet.\\
	
	Zusammengesetzt ergibt sich daraus folgender Code. Da ich den Ablauf der Kommunikation schon in \ref{message} erläutert habe, vereinfache ich das Beispiel an dieser Stelle.\\
	
	\begin{lstlisting}[language=C, caption=Dateninitialisierung]
void field_initializer(u_int8_t *state, u_int32_t *neighbour_matrix) {
	//fills fields with random numbers 0 = dead, 1 = alive
	// use different seed for every rank
	unsigned seed = time(0) + rank;
	// top & bottom
	for (int i = 2; i < block_col - 2; i++) {
		state[block_col + i] = rand_r(&seed) % 2;
		state[(block_row - 2) * block_col + i] = rand_r(&seed) % 2;
	}
	//left & right + corners
	for (int i = 1; i < block_row - 1; i++) {
		state[i * block_col + 1] = rand_r(&seed) % 2;
		state[(i + 1) * block_col - 2] = rand_r(&seed) % 2;
	}
	// MPI_Isend and MPI_Irecv for every edge and corner
	...
	// do the middle, while sending/receiving
	for (int i = 2; i < block_row - 2; i++) {
		for (int j = 2; j < block_col - 2; j++) {
			state[i * block_col + j] = rand_r(&seed) % 2;
		}
	}
	// Wait for all IPC to complete
	MPI_Waitall(16, request, status);
	return;
}\end{lstlisting}
	\begin{figure}[h]
		\begin{center}
			\includegraphics[scale=5.0]{initialized_board.pdf}
		\end{center}
		\caption{Mit \texttt{rand\_r()} generiertes Spielfeld der Größe $64\times 64$}
	\end{figure}
	
	\subsection{Berechnung der nächsten Generation}
	Die Berechnung der nächsten Generation erfolgt mithilfe beider Spielfelder.\\ Die Funktion \texttt{calculate\_next\_gen()} erhält einen Pointer auf das Array mit der aktuellen Generation \texttt{*state\_old}, einen auf das Array der Folgegeneration \texttt{*state} und einen weiteren für die Neighbour Matrix.\\
	Bei jedem Simulationsschritt werden die Pointer getauscht und die Funktion erneut aufgerufen. Damit wird die Forderung der Aufgabenstellung nach \textit{double buffering} erfüllt, sprich die Folgegeneration in einem separatem Spielfeld berechnet.
	\begin{lstlisting}[language=C, caption=Vertauschen der Pointer vor jedem Funktionsaufruf (vereinfacht)]
for (int i = 0; i < repetitions; i++) {
	calculate_next_gen(state_out, state_in, neighbour_matrix);
	state_tmp = state_in;
	state_in = state_out;
	state_out = state_tmp;
}\end{lstlisting}

	\subsubsection{Kanten und Ecken} \label{Kanten}
	Ähnlich der Feldinitialisierung, werden zuerst die Kanten und Ecken berechnet, damit diese im nächsten Schritt gesendet werden können. Da die Kanten jeweils nur aus einer Zeile bzw. Spalte bestehen, wird lediglich eine \texttt{for}-Schleife benötigt.\\
	Ich habe mich willkürlich dazu entschieden, die Ecken mit der linken und rechten Spalte zu berechnen. Man hätte sie aber auch genauso gut mit der obersten und untersten Zeile berechnen können. Vereinfacht habe ich nur die Berechnung der linken und rechten Seite dargestellt.
	\begin{lstlisting}[language=C, caption=Berechnung der Kanten und Ecken]
//left & right + corners
for (int i = 1; i < block_row - 1; i++) {
	u_int8_t sum_of_l_edge = state_old[(i - 1) * block_col] +
	state_old[(i - 1) * block_col + 1] +
	state_old[(i - 1) * block_col + 2] +
	state_old[i * block_col] +
	state_old[i * block_col + 2] +
	state_old[(i + 1) * block_col] +
	state_old[(i + 1) * block_col + 1] +
	state_old[(i + 1) * block_col + 2];
	state[i * block_col + 1] = (sum_of_l_edge == 3) | ((sum_of_l_edge == 2) & state_old[i * block_col + 1]);
	
	u_int8_t sum_of_r_edge = state_old[i * block_col - 3] +
	state_old[i * block_col - 2] +
	state_old[i * block_col - 1] +
	state_old[(i + 1) * block_col - 3] +
	state_old[(i + 1) * block_col - 1] +
	state_old[(i + 2) * block_col - 3] +
	state_old[(i + 2) * block_col - 2] +
	state_old[(i + 2) * block_col - 1];
	
	state[(i + 1) * block_col - 2] =
	(sum_of_r_edge == 3) | ((sum_of_r_edge == 2) & state_old[(i + 1) * block_col - 2]);
}
// top & bottom
for (int i = 2; i < block_col - 2; i++) {
	...
}\end{lstlisting}
	Der Zustand der Zelle in der nächsten Generation wird über die Spielregeln bestimmt und ist abhängig vom aktuellen Zustand der Zelle und ihren acht Nachbarn. Da der Zustand mit Null (tot) oder Eins (lebendig) repräsentiert wird, kann die Zahl der Nachbarzellen aufsummiert werden. Die Summe entspricht dabei der Zahl lebender Nachbarn.\\
	An dieser Stelle könnte mithilfe einer \textit{if}-Verzweigung der Folgezustand entschieden werden. Allerdings entschied ich mich für die Verwendung von bitweisen Operatoren. Es handelt sich dabei aus schaltungstechnischer Sicht um die einfachsten Operationen auf den einzelnen Bits.\\
	Der Grund liegt darin, dass diese bitweisen Operatoren sich gut bei SIMD Operationen verwenden lassen.\\
	\clearpage
	\subsubsection{Inneres Feld}
	Im ersten Schritt werden alle inneren Zellen berechnet. Dafür verwende ich zwei geschachtelte \texttt{for}-Schleifen:\\
	\begin{lstlisting}[language=C, caption=Berechnung der inneren Zellen]
for (int i = 2; i < block_row - 2; i++) {
	for (int j = 2; j < block_col - 2; j++) {
		//count up a number (8)
		u_int8_t sum_of_8 = state_old[(i - 1) * block_col + (j - 1)] +
		state_old[(i - 1) * block_col + j] +
		state_old[(i - 1) * block_col + (j + 1)] +
		state_old[i * block_col + (j - 1)] +
		state_old[i * block_col + (j + 1)] +
		state_old[(i + 1) * block_col + (j - 1)] +
		state_old[(i + 1) * block_col + j] +
		state_old[(i + 1) * block_col + (j + 1)];
		state[i * block_col + j] = (sum_of_8 == 3) | ((sum_of_8 == 2) & state_old[i * block_col + j]);
	}
}\end{lstlisting}

	Diese Berechnung findet während der Kommunikation statt und wird nicht mit anderen ranks geteilt. Die Berechnung der Kanten und Ecken wäre auch in diesem Schritt möglich, in dem man die Zählvariablen um eins früher beginnen und eins später enden lassen würde.\\
	Dann könnte jedoch erst nach Abschluss der Berechnung der Datenaustausch stattfinden, was den Performancevorteil der asynchronen Kommunikation kaputt machen würde.

	\subsection{Zeitmessung}
	Ich habe mich für eine zweistufige Zeitmessung entschieden.\\
	
	In erster Instanz messe ich die Ausführungszeiten der Funktionen \texttt{calculate\_next\_gen()} und \texttt{field\_initializer()}.\\
	Die vergangene Zeit messe ich mit der Funktion \texttt{clock\_gettime()}.\\
	Für die Berechnung der Ausführungszeit wird vor und nach Ausführung der zu untersuchenden Funktion \texttt{clock\_gettime()} aufgerufen. Die Differenz aus den beiden Momentaufnahmen entspricht der jeweiligen Zeit.\\
	
	Zusätzlich messe ich die Ausführungszeit des kompletten Programms mit dem Linux Befehl \textit{time}. Da die Funktionen \texttt{calculate\_next\_gen()} und \texttt{field\_initializer()} den größten Abteil der Programm haben, lässt sich, zumindest näherungsweise, der MPI Overhead zum spawnen und initialisieren der Prozesse berechnen.
	\clearpage
	\begin{lstlisting}[language=C, caption=Berechnung der Ausführungszeit eines \textit{function calls}]
clockid_t clk_id = CLOCK_MONOTONIC;
double time_calc = 0;
struct timespec calc_s, calc_e;
for (int i = 0; i < repetitions; i++) {
	clock_gettime(clk_id, &calc_s);
	// function call
	clock_gettime(clk_id, &calc_e);
	time_calc += (double) (calc_e.tv_nsec-calc_s.tv_nsec) / 1000000000 +
	             (double) (calc_e.tv_sec-calc_s.tv_sec);
}
printf("Calculation took %f seconds to execute.\n", time_calc);
\end{lstlisting}
	
	\subsection{Ein- und Ausgabe}
	Da die Messung später in verschiedenen Feldgrößen durchgeführt wird, habe ich mich für den Einsatz von \verb|getopt| entschieden. Es ermöglicht, die Anzahl der Schleifendurchläufe, die Feldgröße und eine optionale Fortschrittsanzeige über Argumente beim Programmstart einzustellen.\\
	Ebenso lässt sich das Ergebnis über \textit{pbm}-Files visualisieren. Dabei fügt jeder rank seiner Ausgabedatei seinen rank an. Das Bild ließe sich aus diesen Einzelbildern konstruieren, ist jedoch nicht Schwerpunkt dieser Aufgabe.\\
	Alle Funktionen sowie die Syntax lassen sich über den Parameter \texttt{-{}-help} ausgeben.\\
	
	Um mehrfache Ausgaben auf der Konsole zu vermeiden, werden Ausgaben nur vom Prozess mit dem rank 0 ausgeführt.
	\newpage
	\section{Zeitmessung auf Taurus}
	\subsection{Testumgebung} \label{umgebung}
	Alle Messungen wurden auf dem Hochleistungsrechner Taurus der TU Dresden durchgeführt.\\
	Verwendet habe ich die Romeo-Partition, die auf AMD Rome EPYC 7702 Prozessoren basiert \cite{hpc}. Ein EPYC 7702 besteht aus 64 physischen Kernen. \textit{Simultaneous Multithreading} ist deaktiviert. Ein Node verwendet 2 EPYC Prozessoren auf einem Dual-Socket Board und ist mit 512 GB RAM ausgestattet. Als Betriebssystem kommt Centos 7 zum Einsatz.\\
	
	Vor Beginn der Messung muss noch die Topologie des unterliegenden Systems betrachtet werden. Für die optimale Performance sollten die größten Vektorregister zum Einsatz kommen. Bei den EPYC Prozessoren entspricht das AVX2. Die Breite der Register liegt hier bei 256 Bit.\\
	Außerdem wird FMA unterstützt. FMA steht für Fused-multiply-add und steigert die Leistung durch verbesserte Ausnutzung von Registern und einem kompakteren Maschinencode. Das wird durch das Zusammenfassen einer Addition und Multiplikation zu einem Befehl erreicht.
	
	Ich kompilierte das Programm mit dem \texttt{GCC} (\textbf{G}NU \textbf{C}ompiler \textbf{C}ollection, Version 11.2) unter Verwendung von OpenMPI (Version 4.1.2) mit den Compiler-Flags:
	\begin{itemize}
		\item \texttt{O3} - Optimierungsflag des Compilers (u.a. Verwendung von SIMD Instruktionen)
		\item \texttt{mavx2} - Verwendung von AVX2
		\item \texttt{mfma} - Verwendung von FMA
		\item \texttt{lm} - Notwendig für das einbinden der \textit{math library}
	\end{itemize}
	Um die NUMA Eigenschaften optimal zu nutzen, verwendete ich bei der Slurm Ausführung die Parameter:
	\begin{itemize}
		\item \texttt{{-}{-}cpu\_bind=cores
			\item {-}{-}distribution=block:block}
	\end{itemize}
	Der CPU bind bewirkt, dass jedem Prozess ein (physischer) Core zugewiesen wird.\\
	
	Die \textit{distribution} bestimmt die Verteilung der Prozesse über die Sockets und Nodes.
	\newcommand*{\captionsource}[2]{%
		\caption[{#1}]{%
			#1%
			\\\hspace{\linewidth}%
			Quelle: #2%
		}%
	}
	\begin{figure}[h]
		\centering
		\includegraphics[scale=0.5]{mpi_block_block.png}
		\captionsource{Aufteilung der ranks über mehrere Sockets und Nodes mit \texttt{block:block}.}{\href{https://hpc-wiki.zih.tu-dresden.de/jobs\_and\_resources/binding\_and\_distribution\_of\_tasks/}{ZIH HPC Compendium}}
	\end{figure}

	Es werden also zuerst alle Cores innerhalb eines Sockets und anschließend innerhalb eines Nodes verwendet. Je näher die ranks beieinander liegen, desto schneller ist die Kommunikation. Darauf möchte ich unter Betrachtung der AMD EPYC CPU einmal genauer eingehen.
	\subsection{Open MPI Kommunikationskanäle} \label{chan}
	Der Nachrichtenaustausch bei OpenMPI kann dabei über verschiedene Kanäle erfolgen. Die Verfügbaren lassen sich mit dem Befehl\\
	\texttt{ompi\_info | grep 'MCA [mb]tl'}\\
	ausgeben. Auf Taurus bestehen folgende Möglichkeiten:
	\begin{enumerate}
		\item \texttt{self} - Sinnvoll für Kommunikation zum selben rank, beispielsweise bei der Ausführung mit nur einem rank
		\item \texttt{vader} - shared memory communication innerhalb eines Knotens 
		\item \texttt{ofi} - OpenFabrics Interfaces, wichtig für die Verwendung von AMDs \textit{Infinity Fabric}
		\item \texttt{tcp} - Kommunikation über das Netzwerk mittels TCP/IP
	\end{enumerate}
	OpenMPI versucht, bei einer Kommunikation, immer den schnellsten Kommunikationsweg zwischen zwei ranks zu finden.
	\clearpage
	\subsection{AMD EPYC CPU Layout} \label{cpu_layout}
	Bevor ich zur Testmethode und den Ergebnissen übergehe, möchte ich erst auf die Struktur der AMD EPYC CPU eingehen, um Ergebnisse später besser deuten zu können.\\
	
	\begin{figure}[h]
		\centering
		\includegraphics[scale=0.5]{AMD_EPYC.png}
		\captionsource{CPU Layout eines AMD EPYC 7002.}{\href{https://www.amd.com/system/files/documents/amd-epyc-7002-tg-hpc-56827.pdf}{HPC Tuning Guide for AMD EPYC 7002 Series Processors}}
	\end{figure}
	
	Der verwendete AMD EPYC 7702 ist ein 64 Kern Prozessor. Jeder Socket besteht dabei aus 8 \textit{CCDs}, die über ein \textit{Infinity Fabric} untereinander und mit den CCDs des anderen Sockets sehr schnell Daten austauschen können \cite{amd}. Jeder CCD besteht damit aus 8 Cores oder anders ausgedrückt aus 2 \textit{CCX} à 4 Cores.\\
	
	Das zu erwartende Ergebnis ist, dass der Geschwindigkeitsvorteil geringer wird, sobald man mehr als einen CCD (also 8 Cores) benötigt. Bei den durchgeführten Messungen ist das ab 16 ranks der Fall. Ab da müssen die CCDs über Infinity Fabric miteinander kommunizieren, was mehr Zeit benötigen sollte, als die Kommunikation innerhalb eines CCDs.\\
	
	Die nächste interessante Schwelle liegt bei 64 Cores. Sobald sie überschritten wird, müssen die ranks über den Socket hinweg kommunizieren, was aufgrund der längeren Leitungswege ebenfalls Zeit kostet.\\
	
	Mit der Überschreitung von 128 Cores muss dann zwischen zwei Nodes kommuniziert werden. Hier sind die größten Geschwindigkeitseinbrüche zu erwarten, da der Datenaustausch über das Netzwerk im Vergleich zum Datenaustausch auf einem Board höhere Latenzen mit sich bringt.
	\subsection{Testmethode}
	Die Tests wurden automatisiert mit \texttt{sbatch}-Skripten ausgeführt. Um Schwankungen auszugleichen, wurde jede Messung 20 mal wiederholt. Ich allokierte den Knoten für jede Messreihe exklusiv, um Einflüsse durch koexistierende Prozesse auszuschließen.\\
	Da nicht genügend Rechenressourcen zur Verfügung standen, machte ich die Anzahl der Simulationsschritte von der Feldgröße abhängig.\\
	Ich entschied mich dabei für eine exponentielle Abnahme der Wiederholungen, da die Feldgröße exponentiell ansteigt.\\
	
	Dafür testete ich die höchste Anzahl an ranks (256) mit der kleinsten Feldgröße (2048). Die minimale Ausführungszeit sollte bei etwa einer Sekunde (ohne den Overhead von MPI) liegen, um verlässliche Werte zu erhalten. Diese wurde bei 10.000 Simulationsschritten erreicht. Mit der nächst höheren Feldgröße sinkt die Anzahl der Wiederholungen um eine Dekade.\\
	
	Die Messwerte sind daher ungeeignet, um zu beurteilen, wie sich die Ausführungszeit verändert, wenn man die Anzahl der ranks gleich belässt, aber die Feldgröße ändert.\\
	Dass die Ausführungsgeschwindigkeit mit der Feldgröße ansteigt, ist logisch. Interessanter für das Praktikum ist jedoch das Verhalten bei MPI-paralleler Ausführung, was sich beurteilen lässt.\\
	
	Aus den 20 Wiederholungen pro Messung habe ich den Mittelwert gebildet und logarithmische Diagramme erzeugt. Diese Darstellung bietet sich aufgrund der exponentiell ansteigenden Feldgröße und ranks an.
	\begin{description}
		\item[Anmerkung zur Notation:] Mit einer Feldgröße von 128 meine ich ein Feld mit $128\times 128$ Zellen. Da hier im Praktikum alle Feldgrößen quadratisch sind, ist die Angabe der zweiten Größe redundant, weshalb ich auch auf diese verzichte.
		\item[Anmerkung zur exklusiven Nutzung von Knoten:] Aufgrund der begrenzten Rechenressourcen stand ich vor der Entscheidung, Knoten exklusiv zu allokieren oder die Anzahl der Simulationsschritte von der Feldgröße abhängig zu machen.\\
		Ich entschied mich für letzteres, wie oben beschrieben. Warum, zeige ich in \ref{exklusiv}.
	\end{description}

	\newpage
	\section{Testergebnisse} \label{erg}
	In diesem Abschnitt habe ich die Messwerte aus den Tabellen \ref{tables} visualisiert.
	\subsection{Dateninitialisierung}
	Zuerst möchte ich auf die Ausführungszeiten der Funktion \texttt{field\_initializer()} des Spiels eingehen.\\
	
	\begin{figure}[h]
		\centering
		\input{all_init.pgf}
		\caption{Logarithmische Darstellung der Ausführungszeit der Funktion \texttt{field\_initializer()}.}
	\end{figure}
	Die allgemeine Tendenz zeigt, was zu erwarten war: Mehr ranks verbessern die Ausführungsgeschwindigkeit.\\
	Es ist zu erkennen, dass bei 64 Cores für kleine Feldgrößen ein Minimum erreicht wird. Mehr Cores verschlechtern wieder die Performance. Warum? Die Kommunikation nimmt hier mehr Zeit in Anspruch als der eigentliche Vorgang.\\
	Das zeigt sich auch sehr gut daran, dass die Werte der Feldgrößen 2048 und 8192 bei 256 ranks auf einen Punkt fallen. Dadurch wird deutlich, dass der Datenaustausch die Ausführungszeit dominiert und nicht das Füllen der Felder mit Zufallswerten.\\
	Beim Sprung von 64 auf 128 ranks findet eine Kommunikation zwischen den Sockets statt, was die Latenz erhöht. Bei 256 ranks wird ein zweiter Node benötigt. Die Zeit steigt hier dramatisch an. Dieses Verhalten habe ich bereits in \ref{cpu_layout} erklärt.
	\subsection{Berechnung}
	\begin{figure}[h]
		\centering
		\input{all_calc.pgf}
		\caption{Logarithmische Darstellung der Ausführungszeit der Funktion \texttt{calculate\_next\_gen()}.}
	\end{figure}
	Auch hier sinkt die Ausführungszeit mit Anzahl der ranks ab. Es gibt jedoch kein Abknicken, wie noch bei der Initialisierung.\\
	Eine mögliche Begründung dafür ist, dass die Berechnung des inneren Feldes soviel Zeit in Anspruch nimmt, dass währenddessen die asynchrone Kommunikation abgeschlossen wird.\\
	Das ist ein wünschenswertes Verhalten und verdeutlicht, dass das Problem sehr gut mit MPI parallelisiert werden kann.
	\clearpage
	\subsection{Komplettes Programm}
	Hier betrachte ich die benötigte Zeit des Programms, inklusive Spawnen der Prozesse und der Initialisierung von MPI.
	\begin{figure}[h]
		\centering
		\input{all_total.pgf}
		\caption{Logarithmische Darstellung der Ausführungszeit des kompletten Programms.}
	\end{figure}

	Es zeigt sich, dass die Ausführungszeit ab 16 ranks immer stärker vom Overhead durch MPI dominiert wird. Erkennen lässt sich das daran, dass die Ausführungszeiten ab dieser Grenze stagnieren und anschließend wieder steigen. Gleichzeitig rücken die Feldgrößen immer näher zusammen. Das verdeutlicht, dass sie nicht mehr der dominierende Faktor sind.
	\clearpage
	Subtrahiert man von der Ausführungszeit des kompletten Programms die Zeiten der Dateninitialisierung und Berechnung (inklusive der jeweiligen Kommunikation), ergibt sich folgendes Bild.
	\begin{figure}[h]
		\centering
		\input{all_overhead.pgf}
		\caption{Logarithmische Darstellung des ungefähren Overheads.}
	\end{figure}

	Es zeigt in etwa den Overhead durch MPI. Der Overhead sollte nur von der Anzahl der ranks abhängen, da die Initialisierung von MPI und das Spawnen der Prozesse unabhängig von der Problemgröße ist. Das ist erfüllt, da alle Kurven etwa übereinander liegen.\\
	Bei den ranks 1 und 4 ist der Overhead in etwa konstant und im Vergleich zur kompletten Ausführungszeit weniger als 1\%.\\
	Danach steigt er drastisch an, bis er bei 256 ranks schon ca. 90\% ausmacht.\\
	
	Daran sieht man, dass MPI mit steigender Anzahl von Prozessen einen hohen initialen Overhead verursacht. Das Problem muss entsprechend sehr groß sein, um eine gute Gesamtausführungszeit zu erreichen.\\
	\clearpage
	\subsection{Einfluss der exklusiven Nutzung von Knoten}\label{exklusiv}
	\begin{figure}[h]
		\centering
		\input{non_exclusive_overhead.pgf}
		\caption{Logarithmische Darstellung des ungefähren Overheads bei nicht exklusiver Knotennutzung.}
	\end{figure}
	Diese Berechnung entstand auf der Grundlagen von Messwerten, die auf einem Knoten, der nicht exklusiv allokiert wurde, gemessen wurden. Erkennbar ist eine deutlich Streuung der Messwerte, besonders bei den ranks 1 und 4, die nur durch den Einfluss koexistierender Prozesse zu erklären ist.\\
	Aufgrund dessen entschied ich mich, die Messungen mit exklusiven Knoten zu wiederholen, da noch Rechenressourcen vorhanden waren. 
	\newpage
	\appendix
	\section{Tabellen} \label{tables}
	\begin{table}[h]
		\centering
			\begin{tabular}{|| r r r r r ||}
				\hline
				Size & Ranks & Initialization in s & Calculation in s & Total time in s \\ [1ex]
				\hline\hline
				2048 & 1 & 0.0476 & 159.4984 & 162.3866 \\ \hline
				2048 & 4 & 0.0117 & 42.9935 & 45.8009 \\ \hline
				2048 & 16 & 0.0031 & 10.8978 & 15.6966 \\ \hline
				2048 & 64 & 0.001 & 2.8518 & 18.8994 \\ \hline
				2048 & 128 & 0.0013 & 1.458 & 26.6152 \\ \hline
				2048 & 256 & 0.0338 & 0.8018 & 28.8851 \\ \hline \hline
				8192 & 1 & 0.7524 & 278.2817 & 281.8427 \\ \hline
				8192 & 4 & 0.1895 & 66.9046 & 69.9164 \\ \hline
				8192 & 16 & 0.0486 & 16.9429 & 21.6623 \\ \hline
				8192 & 64 & 0.0124 & 4.2742 & 20.4464 \\ \hline
				8192 & 128 & 0.0063 & 2.1142 & 27.151 \\ \hline
				8192 & 256 & 0.0337 & 1.1012 & 29.0222 \\ \hline \hline
				32768 & 1 & 11.9899 & 444.2583 & 459.1336 \\ \hline
				32768 & 4 & 3.0087 & 117.4779 & 123.3376 \\ \hline
				32768 & 16 & 0.7706 & 29.2729 & 34.8168 \\ \hline
				32768 & 64 & 0.1985 & 6.5923 & 22.8972 \\ \hline
				32768 & 128 & 0.0991 & 3.233 & 28.5742 \\ \hline
				32768 & 256 & 0.0833 & 1.66 & 29.7171 \\ \hline \hline
				131072 & 64 & 3.0897 & 11.8205 & 31.104 \\ \hline
				131072 & 128 & 1.5304 & 5.6618 & 32.6177 \\ \hline
				131072 & 256 & 0.796 & 2.8887 & 31.7655 \\ \hline
			\end{tabular}
			\caption{Messergebnisse. \textit{Initialization} und \textit{Calculation} beziehen sich auf die beiden Funktionen \texttt{field\_initializer()} und \texttt{calculate\_next\_gen()}. Zeiten sind Mittelwerte und gerundet auf 4 Nachkommastellen.}
	\end{table}
	\clearpage
	\bibliography{Praktikumsbericht}
\end{document}
